# consultar o arquivo robots.txt dos sites antes de realizar uma raspagem de dados.
Para verificar basta colocar o link/robots.txt (mercadolivre.com/robots.txt)

#Primeiros passos 
1- utilizar o terminal para criar um ambiente virtual com o comando python -m venv iuri
2- ativar ambiente com comando iuri\Scripts\activate
3- instalar o scrapy com pip install scrapy
4- iniciar um projeto scrapy com "scrapy startproject (nome do proejto)"
5- acessar a pasta do projeto
6- criar um arquivo nome_do_site.py dentro da pasta spider 
7- importar bibliotecas conforme codigo abaixo
8- criar um classe em camelcase com um nome do projeto finalizando com spider "NomeProjetoSpider(scrapy.Spider):" 
9- fazer os passos de configurações abaixo: 
Burlar e evitar bloqueios 
- Ir ao arquivo settings.py e alterar: "ROBOTSTXT_OBEY = True" para "ROBOTSTXT_OBEY = False"
- Descomentar a configuração DOWNLOAD_DELAY = 4
- instalar a biblioteca "pip install scrapy-fake-useragent" 
- Colar as configurações abaixo no arquivo settings.py para gerar agentes simulando navegadores: 
    DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,
}

## settings.py

FAKEUSERAGENT_PROVIDERS = [
    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try
    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us
    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value
]

## Set Fallback User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Edg/115.0.1901.203'

#exportar dados para um csv usar o comando "scrapy crawl frasebot -o(comando) dados.csv(nome do arquivo)"

#Codigo inicial: 
    from typing import Any, Iterable
    import scrapy
    from scrapy.http import Response

    class QuotesToScrapeSpider(scrapy.Spider):
        #Identidade (nome do bot)
        name = 'frasebot'
        #Request
        def start_requests(self): 
            urls = ['https://quotes.toscrape.com/']

            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse)

        #Response
        def parse(self, response):
            #Aqui sera feito o processamento das paginas.
            pass


9- executar o codigo com "scrapy crawl frasebot" (frasebot é o nome dado ao bot, pode mudar de acordo com o projeto)

#Xpath 
//div[@class="quote"]/text() #retorna apenas o texto
//div[@class="quote"]/@href #retorna apenas o que esta no atributo href
//div[@class="quote"]/@itentype #retorna apenas o que esta no atributo itentype

para testar o xpath diretamente no scrapy deve ser usado o comando no amibente virtual ativo "scrapy shell https://quotes.toscrape.com/"
para buscar o xpath criado execute o comando "response.xpath('//span[@class="text"]/text()').get()" com o xpath que deseja buscar
para buscar uma lista de resultados usar o comando "response.xpath('//span[@class="text"]/text()').getall()"


#Extrair dados 
Colocar o que deve ser feito na pagina dento da função parse
Codigo exemplo: 
#Response
    def parse(self, response):
        #Aqui sera feito o processamento das paginas
        for elemento in response.xpath("//div[@class='quote']"):
            yield {
                'frase': elemento.xpath(".//span[@class='text']/text()").get(),
                'autor': elemento.xpath(".//small[@class='author']/text()").get(),
                'tags': elemento.xpath(".//a[@class='tag']/text()").getall()
            }



##Limpar e processar dados 
1- Acessar o aquivo ./spiders/items.py
2- 

## Salvar arquivos em outros formatos 
-O para sobrescrever arquivos existentes
-o para acrescentar dados a arquivos existentes(não funciona com json)

scrapy crawl nomebot -O dados.csv
scrapy crawl nomebot -O dados.xml
scrapy crawl nomebot -O dados.json

##Como utilizar proxy caso esteja tendo bloqueios(possui custos):
instalar a biblioteca "pip install scrapeops-scrapy-poxy-sdk" no arquivo settings.py
SCRAPEOPS_API_KEY = 'SUA_CHAVE_API'
SCRAPEOPS_PROXY_ENABLED = True

DOWNLOADER_MIDDLEWARES = {
    'scrapeops_scrapy_proxy_sdk.scrapeops_scrapy_proxy_sdk.ScrapeOpsScrapyProxySdk': 725,
}
